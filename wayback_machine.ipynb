{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook tests which pages from SWDE are available in archive.org. We could then use those because they have also their CSS archived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import json\n",
    "import dataclasses\n",
    "import collections\n",
    "\n",
    "from awe.data import swde, wayback, constants\n",
    "from awe import awe_graph, utils\n",
    "for module in [wayback, utils]:\n",
    "    importlib.reload(module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try only subset for now.\n",
    "pages = [page\n",
    "    for vertical in swde.VERTICALS\n",
    "    for website in vertical.websites\n",
    "    for page in website.pages[:50]]\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWDE_TIMESTAMP = '20110601000000' # SWDE was released in 2011\n",
    "WAYBACK_DATA_PATH = f'{constants.DATA_DIR}/wayback.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved responses.\n",
    "with open(WAYBACK_DATA_PATH, mode='r', encoding='utf-8') as f:\n",
    "    data_dict = json.loads(f.read())\n",
    "already_loaded = 0\n",
    "newly_loaded = 0\n",
    "newly_skipped = 0\n",
    "not_loaded = 0\n",
    "for page in pages:\n",
    "    if page.archived is False:\n",
    "        response = data_dict.get(page.url, False)\n",
    "        if response is None:\n",
    "            # `null` value saved in the JSON means the page was fetched but the\n",
    "            # API returned no results.\n",
    "            page.archived = None\n",
    "            newly_skipped += 1\n",
    "        elif response is False:\n",
    "            # This means the URL is not contained in the JSON.\n",
    "            not_loaded += 1\n",
    "        else:\n",
    "            page.archived = wayback.WaybackPage(page.url, **response)\n",
    "            newly_loaded += 1\n",
    "    else:\n",
    "        already_loaded += 1\n",
    "already_loaded, newly_loaded, newly_skipped, not_loaded, len(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from API.\n",
    "RETRY_FAILED = False\n",
    "PARALLELIZE = -1\n",
    "def filter_page(page: awe_graph.HtmlPage):\n",
    "    return page.archived is False or (page.archived is None and RETRY_FAILED)\n",
    "def fetch_page(page: awe_graph.HtmlPage):\n",
    "    assert not page.archived\n",
    "    page.archived = wayback.WaybackPage.get(page.url, SWDE_TIMESTAMP)\n",
    "pages_to_fetch = list(filter(filter_page, pages))\n",
    "utils.parallelize(PARALLELIZE, fetch_page, pages_to_fetch, 'pages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some stats on data.\n",
    "skipped = 0\n",
    "no_snapshot = 0\n",
    "bad_status = 0\n",
    "total = 0\n",
    "for page in pages:\n",
    "    if page.archived is False:\n",
    "        skipped += 1\n",
    "    elif page.archived is None:\n",
    "        no_snapshot += 1\n",
    "    elif page.archived.status != 200:\n",
    "        bad_status += 1\n",
    "    total += 1\n",
    "skipped, no_snapshot, bad_status, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which websites failed most?\n",
    "failed_websites = collections.defaultdict(int)\n",
    "for page in pages:\n",
    "    if page.archived is None:\n",
    "        failed_websites[page.site.dir_name] += 1\n",
    "list(sorted(failed_websites.items(), key=lambda i: i[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store obtained WaybackMachine API responses in a file.\n",
    "def serialize_page(page: awe_graph.HtmlPage):\n",
    "    if page.archived is None:\n",
    "        return None\n",
    "    d = dataclasses.asdict(page.archived)\n",
    "    del d['original_url']\n",
    "    return d\n",
    "data_dict = data_dict | {\n",
    "    page.url: serialize_page(page)\n",
    "    for page in pages\n",
    "    if page.archived is not False\n",
    "}\n",
    "with open(WAYBACK_DATA_PATH, mode='w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(data_dict, indent=2))\n",
    "len(data_dict), WAYBACK_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
